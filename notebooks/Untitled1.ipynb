{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0154659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardik/Desktop/Research/rss_foresee/utils/utilsJIT.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  factor = torch.tensor(2*3.14157,dtype=torch.float)\n",
      "/home/hardik/Desktop/Research/rss_foresee/utils/utilsJIT.py:6: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  if angle>torch.tensor(3.14157):\n",
      "/home/hardik/Desktop/Research/rss_foresee/utils/utilsJIT.py:6: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if angle>torch.tensor(3.14157):\n",
      "/home/hardik/Desktop/Research/rss_foresee/utils/utilsJIT.py:8: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  if angle<torch.tensor(-3.14157):\n",
      "/home/hardik/Desktop/Research/rss_foresee/utils/utilsJIT.py:8: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if angle<torch.tensor(-3.14157):\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ut_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrobot_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUnicycleJIT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mut_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mut_utilsJIT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ut_utils'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.rcParams['text.usetex'] = True\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "from robot_models.SingleIntegrator2D import *\n",
    "from robot_models.Unicycle import *\n",
    "from robot_models.UnicycleJIT import *\n",
    "\n",
    "from utils.utils import *\n",
    "from ut_utils.ut_utilsJIT import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning) \n",
    "\n",
    "outfile = TemporaryFile()\n",
    "#############################################################\n",
    "# CBF Controller: centralized\n",
    "u1_max = 2.5#2.0# 3.0\n",
    "u2_max = 4.0\n",
    "u1 = cp.Variable((2,1))\n",
    "# delta = cp.Variable((4,1))\n",
    "delta = cp.Variable(1)\n",
    "delta_u = cp.Variable((2,1))\n",
    "u1_ref = cp.Parameter((2,1),value = np.zeros((2,1)) )\n",
    "num_constraints1 = 1 + 3\n",
    "A1 = cp.Parameter((num_constraints1,2),value=np.zeros((num_constraints1,2)))\n",
    "b1 = cp.Parameter((num_constraints1,1),value=np.zeros((num_constraints1,1)))\n",
    "const1 = [A1 @ u1 + b1 + delta * np.array([1,0,0,0]).reshape(-1,1) >= 0]\n",
    "const1 += [ cp.abs( u1[0,0] )  <= u1_max + delta_u[0,0] ]\n",
    "const1 += [ cp.abs( u1[1,0] )  <= u2_max + delta_u[1,0] ]\n",
    "const1 += [ delta_u[0,0] >= 0 ]\n",
    "const1 += [ delta_u[1,0] >= 0 ]\n",
    "objective1 = cp.Minimize( cp.sum_squares( u1 - u1_ref  ) + 100*cp.sum_squares(delta) + 10000 * cp.sum_squares( delta_u ) )\n",
    "cbf_controller = cp.Problem( objective1, const1 )\n",
    "assert cbf_controller.is_dpp()\n",
    "solver_args = {\n",
    "            'verbose': False\n",
    "        }\n",
    "cbf_controller_layer = CvxpyLayer( cbf_controller, parameters=[ u1_ref, A1, b1 ], variables = [u1, delta_u] )\n",
    "\n",
    "# @torch.jit.script\n",
    "def cbf_controller_layer_jit( u1_ref, A1, b1 ):\n",
    "    print(f\"u1:{u1_ref}, A:{A1}, b:{b1}\")\n",
    "    u  = cbf_controller_layer(u1_ref, A1, b1)\n",
    "    print(f\"u soln:{u}\")\n",
    "    return u[0]\n",
    "\n",
    "# u1r = torch.tensor([1.0,1.0]).reshape(-1,1)\n",
    "# A1 = torch.ones((num_constraints1,2))\n",
    "# b1 = torch.zeros((num_constraints1,1))\n",
    "# cbf_controller_layer_jit(u1r, A1, b1)\n",
    "\n",
    "###############################################################\n",
    "def initialize_tensors(follower, leader):\n",
    "    follower.X_torch = torch.tensor( follower.X, requires_grad = True, dtype=torch.float )\n",
    "    leader.X_torch = torch.tensor( leader.X, requires_grad = True, dtype=torch.float )\n",
    "    follower.alpha_torch = torch.tensor(follower.alpha, dtype=torch.float, requires_grad=True)\n",
    "    follower.k_torch = torch.tensor( follower.k, dtype=torch.float, requires_grad = True )\n",
    "    \n",
    "def compute_A1_b1_tensor(robotsJ, robotsK, robotsJ_state, robotsK_state, t, noise):\n",
    "    \n",
    "    x_dot_k_mean, x_dot_k_cov = traced_leader_predict_jit( t, noise )\n",
    "    # print(f\"gp mean: { x_dot_k_mean }, actual_last_xdot: {robotsK.Xdots[:,-1]}\")\n",
    "        \n",
    "    x_dot_k = x_dot_k_mean.T.reshape(-1,1) #+ cov terms??     \n",
    "    A1, b1 = unicycle_SI2D_clf_cbf_fov_evaluator(robotsJ_state, robotsK_state, x_dot_k, robotsJ.k_torch, robotsJ.alpha_torch)\n",
    "   \n",
    "    return A1, b1\n",
    "\n",
    "first_run = True\n",
    "first_generate_sigma_run = True\n",
    "    \n",
    "def get_future_reward( follower, leader, t = 0, noise = torch.tensor(0), enforce_input_constraints = False, leader_predict_function = traced_sigma_point_expand_JIT  ):\n",
    "    # Initialize sigma points for other robots\n",
    "    follower_states = [torch.clone(follower.X_torch)]        \n",
    "    prior_leader_states, prior_leader_weights = initialize_sigma_points2_JIT(leader.X_torch)\n",
    "    leader_states = [prior_leader_states]\n",
    "    leader_weights = [prior_leader_weights]\n",
    "\n",
    "    reward = torch.tensor([0],dtype=torch.float)\n",
    "    tp = t\n",
    "    \n",
    "    maintain_constraints = []\n",
    "    improve_constraints = []    \n",
    "    \n",
    "    for i in range(H):       \n",
    "        \n",
    "        # t0 = time.time()\n",
    "        leader_xdot_states, leader_xdot_weights = leader_predict_function( follower_states[i], leader_states[i], leader_weights[i], torch.tensor(tp), noise ) #traced_sigma_point_expand_JIT\n",
    "        # print(f\"Time 1: {time.time()-t0}\")\n",
    "        \n",
    "        # t0 = time.time()\n",
    "        leader_states_expanded, leader_weights_expanded = traced_sigma_point_scale_up5_JIT( leader_states[i], leader_weights[i])#leader_xdot_weights )\n",
    "        # print(f\"Time 2: {time.time()-t0}\")\n",
    "        \n",
    "        # t0 = time.time()\n",
    "        A, B = traced_unicycle_SI2D_UT_Mean_Evaluator( follower_states[i], leader_states_expanded, leader_xdot_states, leader_weights_expanded, follower.k_torch, follower.alpha_torch )\n",
    "        # print(f\"Time 3: {time.time()-t0}\")    \n",
    "              \n",
    "        # t0 = time.time()\n",
    "        leader_mean_position = traced_get_mean_JIT( leader_states[i], leader_weights[i] )  \n",
    "        # print(f\"Time 4: {time.time()-t0}\")\n",
    "              \n",
    "        # print(f\"leader_mean:{leader_mean_position.T}, follower:{ follower_states[-1].T }\")\n",
    "        u_ref = traced_unicycle_nominal_input_tensor_jit( follower_states[i], leader_mean_position )\n",
    "        \n",
    "        # t0 = time.time()\n",
    "        solution, deltas = cbf_controller_layer( u_ref, A, B )\n",
    "        # print(\"solution\", solution)\n",
    "        # print(f\"Time 5: {time.time()-t0}\")\n",
    "        if enforce_input_constraints:\n",
    "            if np.any( deltas.detach().numpy() > 0.01 ):\n",
    "                # cannot satisfy with input constraints\n",
    "                improve_constraints.append( -B[0] ) # increase B. reduce -B\n",
    "                improve_constraints.append( -B[1] ) # increase B. reduce -B\n",
    "                improve_constraints.append( -B[2] ) # increase B. reduce -B\n",
    "                improve_constraints.append( -B[3] ) # increase B. reduce -B\n",
    "                if deltas[0,0] > 0.01:\n",
    "                    improve_constraints.append( deltas[0,0] )\n",
    "                if deltas[1,0] > 0.01:\n",
    "                    improve_constraints.append( deltas[1,0] )\n",
    "                print(f\"Infeasible solution found at :{i}. Will improve first\")\n",
    "                # exit()\n",
    "                return maintain_constraints, improve_constraints, False, reward\n",
    "            else:\n",
    "                temp = A @ solution + B\n",
    "                # maintain_constraints.append(temp[0] + 0.01)\n",
    "                maintain_constraints.append(temp[1] + 0.01)\n",
    "                maintain_constraints.append(temp[2] + 0.01)\n",
    "                maintain_constraints.append(temp[3] + 0.01)\n",
    "                # if np.any( temp[1:].detach().numpy() < 0 ):\n",
    "                #     print(\"Issue here\")\n",
    "            \n",
    "        follower_states.append( follower.step_torch( follower_states[i], solution, dt_outer ) )        \n",
    "        leader_next_state_expanded = leader_states_expanded + leader_xdot_states * dt_outer\n",
    "        \n",
    "        # t0 = time.time()\n",
    "        leader_next_states, leader_next_weights = traced_sigma_point_compress_JIT( leader_next_state_expanded, leader_xdot_weights )        \n",
    "        # print(f\"Time 6: {time.time()-t0}\")\n",
    "        leader_states.append( leader_next_states ); leader_weights.append( leader_next_weights )\n",
    "        \n",
    "        # Get reward for this state and control input choice = Expected reward in general settings\n",
    "        # t0 = time.time()\n",
    "        reward = reward + traced_unicycle_reward_UT_Mean_Evaluator_basic( follower_states[i+1], leader_states[i+1], leader_weights[i+1])\n",
    "        # print(f\"Time 7: {time.time()-t0}\")\n",
    "        \n",
    "        tp = tp + dt_outer\n",
    "\n",
    "    return maintain_constraints, improve_constraints, True, reward\n",
    "        \n",
    "################################################################\n",
    "\n",
    "# Sim Parameters\n",
    "num_steps = 300#100#50#20#100#50 #100 #200 #200\n",
    "learn_period = 1#2\n",
    "gp_training_iter_init = 30\n",
    "train_gp = False\n",
    "outer_loop = 2\n",
    "H = 30# 5\n",
    "gp_training_iter = 10\n",
    "d_min = 0.3\n",
    "d_max = 2.0\n",
    "angle_max = np.pi/3\n",
    "num_points = 5\n",
    "dt_inner = 0.01\n",
    "dt_outer = 0.05 #0.1\n",
    "alpha_cbf = 0.3 #1.0#0.1 # 0.5   # Initial CBF\n",
    "k_clf = 1\n",
    "num_robots = 1\n",
    "lr_alpha = 0.05 #0.05\n",
    "max_history = 100\n",
    "print_status = False\n",
    "\n",
    "follower_init_pose = np.array([0,0,np.pi*0.0])\n",
    "leader_init_pose = np.array([0.4,0])\n",
    "\n",
    "\n",
    "plot_x_lim = (0,6)  #(0,10)\n",
    "plot_y_lim = (-3,3) #(-4,4)\n",
    "\n",
    "\n",
    "\n",
    "def constrained_update( objective, maintain_constraints, improve_constraints, params ) :\n",
    "    \n",
    "    \n",
    "    num_params = 4\n",
    "    d = cp.Variable((num_params,1))\n",
    "    \n",
    "    # Get Performance optimal direction\n",
    "    try:\n",
    "        objective.sum().backward(retain_graph = True) \n",
    "        k_grad = getGrad(params[0], l_bound = -20.0, u_bound = 20.0 )\n",
    "        alpha_grad = getGrad(params[1], l_bound = -20.0, u_bound = 20.0 )\n",
    "        objective_grad = np.append( k_grad.reshape(1,-1), alpha_grad.reshape(1,-1), axis = 1 )\n",
    "    except:\n",
    "        objective_grad = np.array([[0,0,0,0]])\n",
    "    \n",
    "    # Get constraint improve direction # assume one at a time\n",
    "    improve_constraint_direction = np.array([0,0,0,0]).reshape(1,-1)\n",
    "    for i, constraint in enumerate( improve_constraints):\n",
    "        constraint.sum().backward(retain_graph=True)\n",
    "        k_grad = getGrad(params[0], l_bound = -20.0, u_bound = 20.0 )\n",
    "        alpha_grad = getGrad(params[1], l_bound = -20.0, u_bound = 20.0 )\n",
    "        improve_constraint_direction = improve_constraint_direction +  np.append( k_grad.reshape(1,-1), alpha_grad.reshape(1,-1), axis = 1 )\n",
    "    \n",
    "    # Get allowed directions\n",
    "    N = len(maintain_constraints)\n",
    "    if N>0:\n",
    "        d_maintain = np.zeros((N,num_params))#cp.Variable( (N, num_params) )\n",
    "        constraints = []\n",
    "        for i, constraint in enumerate(maintain_constraints):\n",
    "            constraint.sum().backward(retain_graph=True)\n",
    "            k_grad = getGrad(params[0], l_bound = -20.0, u_bound = 20.0 )\n",
    "            alpha_grad = getGrad(params[1], l_bound = -20.0, u_bound = 20.0 )\n",
    "            d_maintain[i,:] = np.append( k_grad.reshape(1,-1), alpha_grad.reshape(1,-1), axis = 1 )[0]\n",
    "            \n",
    "            if constraints ==[]: \n",
    "                constraints = constraint.detach().numpy().reshape(-1,1)\n",
    "            else:\n",
    "                constraints = np.append( constraints, constraint.detach().numpy().reshape(-1,1), axis = 0 )       \n",
    "\n",
    "        const = [ constraints + d_maintain @ d >= 0 ]\n",
    "        const += [ cp.sum_squares( d ) <= 200 ]\n",
    "        if len(improve_constraint_direction)>0:\n",
    "            obj = cp.Minimize( improve_constraint_direction @ d )\n",
    "        else:\n",
    "            obj = cp.Minimize(  objective_grad @ d  )\n",
    "        problem = cp.Problem( obj, const )    \n",
    "        problem.solve( solver = cp.GUROBI )    \n",
    "        if problem.status != 'optimal':\n",
    "            print(\"Cannot Find feasible direction\")\n",
    "            exit()\n",
    "        \n",
    "        # print(\"update direction: \", d.value.T)\n",
    "        \n",
    "        return d.value\n",
    "    \n",
    "    else:\n",
    "        if len( improve_constraints ) > 0:\n",
    "            obj = cp.Maximize( improve_constraint_direction @ d )\n",
    "            # print(\"update direction: \", -improve_constraint_direction.reshape(-1,1).T)\n",
    "            return -improve_constraint_direction.reshape(-1,1)\n",
    "        else:\n",
    "            return -objective_grad.reshape(-1,1)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def simulate_scenario(movie_name = 'test.mp4', adapt = False, noise = 0.1, enforce_input_constraints = False, leader_predict_function = traced_sigma_point_expand_JIT):\n",
    "\n",
    "    t = 0\n",
    "    first_time = True\n",
    "    plt.ion()\n",
    "    fig = plt.figure()\n",
    "    # Plotting             \n",
    "    \n",
    "    ax = plt.axes(xlim=plot_x_lim,ylim=plot_y_lim)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    follower = Unicycle(follower_init_pose, dt_inner, ax, num_robots=num_robots, id = 0, min_D = d_min, max_D = d_max, FoV_angle = angle_max, color='g',palpha=1.0, alpha=alpha_cbf, k = k_clf, num_alpha = 3 )\n",
    "    leader = SingleIntegrator2D(leader_init_pose, dt_inner, ax, color='r',palpha=1.0, target = 0 )\n",
    "\n",
    "    metadata = dict(title='Movie Adapt 0', artist='Matplotlib',comment='Movie support!')\n",
    "    writer = FFMpegWriter(fps=15, metadata=metadata)\n",
    "\n",
    "    step_rewards_adapt = []\n",
    "    follower.states_array = np.copy( follower.X )\n",
    "    leader.states_array = np.copy( leader.X )\n",
    "\n",
    "    with writer.saving(fig, movie_name, 100): \n",
    "\n",
    "        for i in range(num_steps):\n",
    "\n",
    "            # High frequency\n",
    "            if i % outer_loop != 0 or i<learn_period:\n",
    "            \n",
    "                uL, vL = leader_motion(t)\n",
    "                u_leader = np.array([ uL, vL ]).reshape(-1,1)\n",
    "                \n",
    "                leader.step(u_leader, dt_inner)\n",
    "                \n",
    "                # implement controller\n",
    "                initialize_tensors(follower, leader)\n",
    "                u_ref = unicycle_nominal_input_tensor_jit( follower.X_torch, leader.X_torch )\n",
    "                A, B = compute_A1_b1_tensor( follower, leader, follower.X_torch, leader.X_torch, torch.tensor(t), torch.tensor(noise) )\n",
    "                \n",
    "                solution, deltas = cbf_controller_layer( u_ref, A, B )\n",
    "                if np.any( deltas.detach().numpy() > 0.01 ) and enforce_input_constraints:\n",
    "                    print(\"Solution failed\")\n",
    "                    return fig, ax, follower, leader, step_rewards_adapt\n",
    "                    solution = solution.detach().numpy()\n",
    "                    solution[0,0] = np.clip( solution[0,0], -u1_max, u1_max )\n",
    "                    solution[1,0] = np.clip( solution[1,0], -u2_max, u2_max )\n",
    "                else:\n",
    "                    # print(f\"u_follower:{solution}\")\n",
    "                    follower.step(solution.detach().numpy(), dt_inner)\n",
    "                follower.ks_u = np.append( follower.ks_u, follower.k )\n",
    "                follower.alphas_u = np.append( follower.alphas_u, follower.alpha, axis=1 )\n",
    "                \n",
    "                # print(f\"reward computation: f:{ follower.X.T }, L:{leader.X.T}\")\n",
    "                step_rewards_adapt.append( unicycle_compute_reward_jit(torch.tensor(follower.X),torch.tensor(leader.X)).detach().numpy()[0,0] )\n",
    "                \n",
    "                t = t + dt_inner\n",
    "                \n",
    "            # Low Frequency tuning\n",
    "            else: \n",
    "                \n",
    "                if adapt:\n",
    "                    initialize_tensors(follower, leader)\n",
    "                    \n",
    "                    success = False\n",
    "                    while not success:\n",
    "                        maintain_constraints, improve_constraints, success, reward = get_future_reward( follower, leader, t = t, noise = torch.tensor(noise), enforce_input_constraints = enforce_input_constraints, leader_predict_function = traced_sigma_point_expand_JIT)                    \n",
    "                        grads = constrained_update( reward, maintain_constraints, improve_constraints, [follower.k_torch, follower.alpha_torch] )\n",
    "                        \n",
    "                        grads = np.clip( grads, -2.0, 2.0 )\n",
    "                        follower.k = np.clip(follower.k + lr_alpha * grads[0], 0.0, None )\n",
    "                        follower.alpha = np.clip( follower.alpha + lr_alpha * grads[1:].reshape(-1,1), 0.0, None )\n",
    "                        print(f\"follower k:{follower.k}, alpha:{follower.alpha.T}\")\n",
    "                        follower.ks = np.append( follower.ks, follower.k )\n",
    "                        follower.alphas = np.append( follower.alphas, follower.alpha, axis=1 )\n",
    "                        initialize_tensors(follower, leader)\n",
    "                    # print(\"Successfully made it feasible\")      \n",
    "                    # exit()     \n",
    "                        \n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "            writer.grab_frame()\n",
    "           \n",
    "    # return data for plotting \n",
    "    return fig, ax, follower, leader, step_rewards_adapt\n",
    "  \n",
    "## Without noise: perfect knowledge??\n",
    "\n",
    "noise  = 1.0 #0.5\n",
    "save_plot = False\n",
    "\n",
    "fig1, ax1, follower1, leader1, rewards1 = simulate_scenario( movie_name = 'noise_no_bound_no_adapt.mp4', adapt = False, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_JIT )  \n",
    "fig2, ax2, follower2, leader2, rewards2 = simulate_scenario( movie_name = 'noise_adapt_no_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_JIT )  \n",
    "fig3, ax3, follower3, leader3, rewards3 = simulate_scenario( movie_name = 'noise_no_adapt_with_bound.mp4', adapt = False, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_JIT )\n",
    "fig4, ax4, follower4, leader4, rewards4 = simulate_scenario( movie_name = 'noise_adapt_with_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_JIT )\n",
    "\n",
    "fig5, ax5, follower5, leader5, rewards5 = simulate_scenario( movie_name = 'ideal_no_bound_no_adapt.mp4', adapt = False, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_ideal_JIT )  \n",
    "fig6, ax6, follower6, leader6, rewards6 = simulate_scenario( movie_name = 'ideal_adapt_no_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_ideal_JIT )  \n",
    "fig7, ax7, follower7, leader7, rewards7 = simulate_scenario( movie_name = 'ideal_no_adapt_with_bound.mp4', adapt = False, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_ideal_JIT )\n",
    "fig8, ax8, follower8, leader8, rewards8 = simulate_scenario( movie_name = 'ideal_adapt_with_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_ideal_JIT )\n",
    "\n",
    "fig9, ax9, follower9, leader9, rewards9 = simulate_scenario( movie_name = 'nominal_no_bound_no_adapt.mp4', adapt = False, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_nominal_JIT )  \n",
    "fig10, ax10, follower10, leader10, rewards10 = simulate_scenario( movie_name = 'nominal_adapt_no_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=False, leader_predict_function = traced_sigma_point_expand_nominal_JIT )  \n",
    "fig11, ax11, follower11, leader11, rewards11 = simulate_scenario( movie_name = 'nominal_no_adapt_with_bound.mp4', adapt = False, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_nominal_JIT )\n",
    "fig12, ax12, follower12, leader12, rewards12 = simulate_scenario( movie_name = 'nominal_adapt_with_bound.mp4', adapt = True, noise = 0.0, enforce_input_constraints=True, leader_predict_function = traced_sigma_point_expand_nominal_JIT )\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "plt.ioff()\n",
    "\n",
    "h11 = []; h21 = []\n",
    "h12 = []; h22 = []\n",
    "h13 = []; h23 = []\n",
    "\n",
    "def get_barriers( follower, leader ):\n",
    "    h1s = []; h2s = []; h3s = []\n",
    "    for i in range( follower.Xs.shape[1] ):\n",
    "        h1, h2, h3 = unicycle_SI2D_fov_barrier( follower.Xs[:,i].reshape(-1,1), leader.Xs[:,i].reshape(-1,1) )\n",
    "        h1s.append(h1); h2s.append(h2); h3s.append(h3)\n",
    "    tp = np.linspace( 0, dt_inner * follower.Xs.shape[1], follower.Xs.shape[1]  )\n",
    "    return tp, h1s, h2s, h3s\n",
    "    \n",
    "tp1, h11, h12, h13 = get_barriers( follower1, leader1 )    \n",
    "tp2, h21, h22, h23 = get_barriers( follower2, leader2 )\n",
    "tp3, h31, h32, h33 = get_barriers( follower3, leader3 )\n",
    "tp4, h41, h42, h43 = get_barriers( follower4, leader4 )\n",
    "tp5, h51, h52, h53 = get_barriers( follower5, leader5 )    \n",
    "tp6, h61, h62, h63 = get_barriers( follower6, leader6 )\n",
    "tp7, h71, h72, h73 = get_barriers( follower7, leader7 )\n",
    "tp8, h81, h82, h83 = get_barriers( follower8, leader8 )\n",
    "tp9, h91, h92, h93 = get_barriers( follower9, leader9 )    \n",
    "tp10, h101, h102, h103 = get_barriers( follower10, leader10 )\n",
    "tp11, h111, h112, h113 = get_barriers( follower11, leader11 )\n",
    "tp12, h121, h122, h123 = get_barriers( follower12, leader12 )\n",
    "\n",
    "\n",
    "print(f\"rewards1:{rewards12}\")\n",
    "   \n",
    "# Plot\n",
    "\n",
    "print(\"Plotting now\")\n",
    "\n",
    "def plot_results(follower1, follower2, follower3, follower4, leader1, leader2, leader3, leader4, rewards1, rewards2, rewards3, rewards4, namespace=\"noise\"):\n",
    "    \n",
    "    tp1, h11, h12, h13 = get_barriers( follower1, leader1 )    \n",
    "    tp2, h21, h22, h23 = get_barriers( follower2, leader2 )\n",
    "    tp3, h31, h32, h33 = get_barriers( follower3, leader3 )\n",
    "    tp4, h41, h42, h43 = get_barriers( follower4, leader4 )\n",
    "\n",
    "    figure1, axis1 = plt.subplots( 1 , 1)\n",
    "    axis1.plot(tp1,h11,'r',label='Unbounded - Fixed', alpha = 0.3)\n",
    "    axis1.plot(tp2,h21,'r.',label='Unbounded - Adaptive')\n",
    "    axis1.plot(tp3,h31,'r--',label='Bounded - Fixed')\n",
    "    axis1.plot(tp4,h41,'r',label='Bounded - Adapt')\n",
    "\n",
    "    axis1.plot(tp1,h12,'g',label='Unbounded - Fixed', alpha = 0.3)\n",
    "    axis1.plot(tp2,h22,'g.',label='Unbounded - Adaptive')\n",
    "    axis1.plot(tp3,h32,'g--',label='Bounded - Fixed')\n",
    "    axis1.plot(tp4,h42,'g',label='Bounded - Adapt')\n",
    "\n",
    "    axis1.plot(tp1,h13,'k',label='Unbounded - Fixed', alpha = 0.3)\n",
    "    axis1.plot(tp2,h23,'k.',label='Unbounded - Adaptive')\n",
    "    axis1.plot(tp3,h33,'k--',label='Bounded - Fixed')\n",
    "    axis1.plot(tp4,h43,'k',label='Bounded - Adapt')\n",
    "    \n",
    "    axis1.set_ylim(-0.2,4.0)\n",
    "\n",
    "    # axis1.set_title('Follower barriers 1 alphas')\n",
    "    axis1.set_xlabel('time (s)')\n",
    "    axis1.legend()\n",
    "    axis1.grid()\n",
    "\n",
    "    figure2, axis2 = plt.subplots( 2 , 1)\n",
    "    axis2[0].plot(tp1,follower1.Us[0,:],'r',label='Unbounded - Fixed', alpha = 1.0)\n",
    "    axis2[0].plot(tp2,follower2.Us[0,:],'b',label='Unbounded - Adaptive')\n",
    "    axis2[0].plot(tp3,follower3.Us[0,:],'k',label='Bounded - Fixed')\n",
    "    axis2[0].plot(tp4,follower4.Us[0,:],'g',label='Bounded - Adapt')\n",
    "    axis2[0].axhline(y = u1_max, color = 'k', linestyle='--')\n",
    "    axis2[0].set_ylabel('$u$')\n",
    "    axis2[0].grid()\n",
    "    axis2[0].set_ylim(0.0,3.5)\n",
    "\n",
    "    axis2[1].plot(tp1,follower1.Us[1,:],'r',label='Unbounded - Fixed', alpha = 0.3)\n",
    "    axis2[1].plot(tp2,follower2.Us[1,:],'b',label='Unbounded - Adaptive')\n",
    "    axis2[1].plot(tp3,follower3.Us[1,:],'k',label='Bounded - Fixed')\n",
    "    axis2[1].plot(tp4,follower4.Us[1,:],'g',label='Bounded - Adapt')\n",
    "    axis2[1].set_ylabel(r'$\\omega$')\n",
    "    axis2[1].axhline(y = u2_max, color = 'k', linestyle='--')\n",
    "    axis2[1].grid()\n",
    "    axis2[1].set_ylim(-5.0,5.0)\n",
    "\n",
    "    # axis1.set_title('Follower barriers 1 alphas')\n",
    "    axis2[1].set_xlabel('time (s)')\n",
    "    axis2[1].legend()\n",
    "\n",
    "    figure3, axis3 = plt.subplots( 1 , 1)\n",
    "    tp1 = np.linspace( 0, dt_inner * len(rewards1), len(rewards1)  )\n",
    "    tp2 = np.linspace( 0, dt_inner * len(rewards2), len(rewards2)  )\n",
    "    tp3 = np.linspace( 0, dt_inner * len(rewards3), len(rewards3)  )\n",
    "    tp4 = np.linspace( 0, dt_inner * len(rewards4), len(rewards4)  )\n",
    "    axis3.plot(tp1,-rewards1,'k',label='Unbounded - Fixed', alpha = 1.0)\n",
    "    axis3.plot(tp2,-rewards2,'r',label='Unbounded - Adaptive', alpha = 1.0)\n",
    "    axis3.plot(tp3,-rewards3,'g',label='Bounded - Fixed', alpha = 1.0)\n",
    "    axis3.plot(tp4,-rewards4,'c',label='Bounded - Adapt', alpha = 1.0)\n",
    "    axis3.set_xlabel('time (s)')\n",
    "    axis3.legend()\n",
    "    axis3.set_ylabel('Rewards')\n",
    "    axis3.grid()\n",
    "    axis3.set_ylim(0.9,2.2)\n",
    "\n",
    "    figure4, axis4 = plt.subplots( 2 , 1)\n",
    "    tp1 = np.linspace( 0, dt_inner * follower2.alphas_u.shape[1], follower2.alphas_u.shape[1]  )\n",
    "    tp2 = np.linspace( 0, dt_inner * follower2.alphas_u.shape[1], follower2.alphas_u.shape[1]  )\n",
    "    tp3 = np.linspace( 0, dt_inner * follower2.alphas_u.shape[1], follower2.alphas_u.shape[1]  )\n",
    "    tp4 = np.linspace( 0, dt_inner * follower2.alphas_u.shape[1], follower2.alphas_u.shape[1]  )\n",
    "    axis4[0].plot(tp4,follower2.ks_u,'c',label=r'$\\alpha_0$', alpha = 1.0)\n",
    "    axis4[0].plot(tp1,follower2.alphas_u[0,:],'k',label=r'$\\alpha_1$', alpha = 1.0)\n",
    "    axis4[0].plot(tp2,follower2.alphas_u[1,:],'r',label=r'$\\alpha_2$', alpha = 1.0)\n",
    "    axis4[0].plot(tp3,follower2.alphas_u[2,:],'g',label=r'$\\alpha_3$', alpha = 1.0)\n",
    "    # axis4[0].set_title('Unbounded Input with Adaptive')\n",
    "    axis4[0].grid()\n",
    "    axis4[0].legend()\n",
    "    axis4[0].set_ylim(-0.1, 2.0)\n",
    "\n",
    "    axis4[1].plot(tp4,follower4.ks_u,'c',label=r'$\\alpha_0$', alpha = 1.0)\n",
    "    axis4[1].plot(tp1,follower4.alphas_u[0,:],'k',label=r'$\\alpha_1$', alpha = 1.0)\n",
    "    axis4[1].plot(tp2,follower4.alphas_u[1,:],'r',label=r'$\\alpha_2$', alpha = 1.0)\n",
    "    axis4[1].plot(tp3,follower4.alphas_u[2,:],'g',label=r'$\\alpha_3$', alpha = 1.0)\n",
    "    # axis4[1].set_title('Bounded Adaptive')\n",
    "    axis4[1].set_xlabel('time (s)')\n",
    "    axis4[1].grid()\n",
    "    axis4[1].legend()\n",
    "    axis4[1].set_ylim(-0.1, 20)\n",
    "    \n",
    "    if save_plot:\n",
    "        figure1.savefig(namespace+\"barriers.eps\")\n",
    "        figure1.savefig(namespace+\"barriers.png\")\n",
    "        figure2.savefig(namespace+\"control.eps\")\n",
    "        figure2.savefig(namespace+\"control.png\")\n",
    "        figure3.savefig(namespace+\"rewards.eps\")\n",
    "        figure3.savefig(namespace+\"rewards.png\")\n",
    "        figure4.savefig(namespace+\"params.eps\")\n",
    "        figure4.savefig(namespace+\"params.png\")\n",
    "   \n",
    "plot_results( follower1, follower2, follower3, follower4, leader1, leader2, leader3, leader4, np.asarray(rewards1), np.asarray(rewards2), np.asarray(rewards3), np.asarray(rewards4),  namespace=\"noise\" )\n",
    "plot_results( follower5, follower6, follower7, follower8, leader5, leader6, leader7, leader8, np.asarray(rewards5), np.asarray(rewards6), np.asarray(rewards7), np.asarray(rewards8),  namespace=\"ideal\" )\n",
    "plot_results( follower9, follower10, follower11, follower12, leader9, leader10, leader11, leader12, np.asarray(rewards9), np.asarray(rewards10), np.asarray(rewards11), np.asarray(rewards12),  namespace=\"nominal\" )\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c574a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
